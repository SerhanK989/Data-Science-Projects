{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0: Loading Dependencies\n",
    "\n",
    "We load sagemaker because thats what will be doing our computation and we need it to interface with AWS. Scipy, numpy, pandas, and the sklearn cosine similarity function as they will help with our computations and processing. Jikan lets us interface with myanimelist. Boto3 and json are there to help us interface with sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from jikanpy import Jikan\n",
    "import boto3, json, io\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load our animelist file to a dataframe, where we'll do some preprocessing and get some information about our dataset, namely the number of users and shows. Then we'll also split out data into a test and train set.\n",
    "\n",
    "I download this data from: https://www.kaggle.com/azathoth42/myanimelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animeList = pd.read_csv('animelists_cleaned.csv', usecols = ['username', 'anime_id', 'my_score'])\n",
    "nAnime = animeList['anime_id'].nunique()\n",
    "nUsers = animeList['username'].nunique()\n",
    "animeList = animeList[animeList.my_score != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nAnime, nUsers)\n",
    "animeList.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to scale all of our users ratings around their respective average rating. This is just because a user who has an average rating of a 5/10 and another user who averages a 7/10 rating should be treated the same. From here one, we'll use scaledList as our main source of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanRating = animeList.groupby('username').mean()\n",
    "MeanRating['meanRating'] = MeanRating['my_score']\n",
    "MeanRating.drop(['anime_id', 'my_score'], axis = 1, inplace = True)\n",
    "\n",
    "stdRating = animeList.groupby('username').std(ddof = 0)\n",
    "stdRating['stdRating'] = stdRating['my_score']\n",
    "\n",
    "\n",
    "stdRating.drop(['anime_id', 'my_score'], axis = 1, inplace = True)\n",
    "stdRating['stdRating'] = stdRating['stdRating'].replace(to_replace = 0.0, value = 1.0)\n",
    "MSList = pd.merge(stdRating, MeanRating, on = 'username')\n",
    "\n",
    "scaledList = pd.merge(animeList, MSList, on = 'username')\n",
    "scaledList['my_score'] = (scaledList['my_score'] - scaledList['meanRating'])/(scaledList['stdRating'])\n",
    "scaledList.drop(['stdRating', 'meanRating'], axis = 1, inplace = True)\n",
    "\n",
    "del(MeanRating)\n",
    "del(stdRating)\n",
    "del(animeList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now scaledList is the only list we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nColsUsed = len(scaledList.index)\n",
    "\n",
    "nFeatures = nAnime +nUsers\n",
    "nTrain = int(nColsUsed*.95)\n",
    "nTest = 958432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note is that there are significantly more shows than people are watching. The maximum animeId is arund 30,000, but in reality there are only around 7000 shows people have watched. In a sparse format this isn't a big deal. But later we will have to convert it to an array to pass it to AWS. At that point in time, it will be important that we reduce the number of columns we have. So we write a function that returns a dictionary mapping Id's to their respective 'scaled' values.\n",
    "\n",
    "We need a similar map from usernames to userIds too (as our dataset doesnt give us userIds) in order to load our info into our dataset.\n",
    "\n",
    "We could do this step when we're loading the data from the dataset into our sparse matrix. In the interest of modularity though, we do it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnimeDict(animeList):\n",
    "    rows = animeList.itertuples()\n",
    "    animeDict = {}\n",
    "    numAnime = 0\n",
    "    for row in rows:\n",
    "        \n",
    "        if row[2] not in animeDict:\n",
    "            animeDict[row[2]] = numAnime\n",
    "            numAnime += 1\n",
    "    return animeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animeMap = getAnimeDict(scaledList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserDict(animeList):\n",
    "    rows = animeList.itertuples()\n",
    "    userDict = {}\n",
    "    numUsers = 0\n",
    "    for row in rows:\n",
    "        if row[1] not in userDict:\n",
    "            userDict[row[1]] = numUsers\n",
    "            numUsers += 1\n",
    "    return userDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userMap = getUserDict(scaledList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to load our test and train data as a sparse matrices then upload them to AWS. Despite the fact that its sparse, its still pretty huge, so this takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(pdList, lines, columns, offset, userMap, animeMap):\n",
    "    X = sp.sparse.lil_matrix((lines, columns)).astype('float32')\n",
    "    Y = []\n",
    "    numUsers = 0\n",
    "    UserDict = {}\n",
    "    F = pdList[offset:].itertuples()\n",
    "    #print ('starting')\n",
    "    i = offset\n",
    "    \n",
    "    for row in F:\n",
    "        if i % 1000000 == 0: print(i)\n",
    "        if i-offset >= lines: break\n",
    "        \n",
    "        username = row[1]\n",
    "        animeId = row[2]\n",
    "        rating = row[3]\n",
    "        \n",
    "        userId = userMap[username]\n",
    "        adjustedAnimeId = animeMap[animeId]\n",
    "            \n",
    "        X[i-offset,int(userId)-1] = 1\n",
    "        X[i-offset,int(nUsers)+int(adjustedAnimeId)-1] = 1\n",
    "        if rating >= 0.0:\n",
    "            Y.append(1)\n",
    "        else:\n",
    "            Y.append(0)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset(scaledList, nTrain, nFeatures, 0, userMap, animeMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = loadDataset(scaledList, nTest , nFeatures, nTrain, userMap, animeMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[0:5])\n",
    "print(X_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Machine Learning on Sagemaker\n",
    "\n",
    "Now we upload stuff to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'YOUR OUTPUT PATH'\n",
    "\n",
    "train_key = 'train.protobuf'\n",
    "train_prefix = '{}/train'.format(output_path)\n",
    "\n",
    "test_key = 'test.protobuf'\n",
    "test_prefix = '{}/test'.format(output_path)\n",
    "\n",
    "bucket = 'YOUR BUCKET NAME'\n",
    "\n",
    "outputPath = 's3://{}/{}/output'.format(bucket, output_path)\n",
    "\n",
    "access_key_id = 'YOUR AWS_ACCESS KEY ID'\n",
    "secret_access_key = 'YOUR SECRET ACCESS KEY'\n",
    "\n",
    "regionName = 'YOUR REGION NAME'\n",
    "\n",
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest'}\n",
    "\n",
    "client = boto3.client('runtime.sagemaker', region_name=regionName, aws_access_key_id = access_key_id,\n",
    "                          aws_secret_access_key= secret_access_key)\n",
    "\n",
    "boto3Session = boto3.Session( aws_access_key_id = access_key_id,\n",
    "                          aws_secret_access_key= secret_access_key,\n",
    "                    region_name = regionName)\n",
    "\n",
    "AWSSession = sagemaker.session.Session(boto_session = sess1)\n",
    "\n",
    "AWSSession.sagemaker_runtime_client = client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key, session):\n",
    "    buf = io.BytesIO()\n",
    "    sagemaker.amazon.common.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    \n",
    "    session.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key, AWSSession)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key, AWSSession) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = sagemaker.estimator.Estimator(containers[AWSSession.boto_session.region_name],\n",
    "                                   train_instance_count=1,\n",
    "                                   role = \"arn:aws:iam::915797848381:role/service-role/AmazonSageMaker-ExecutionRole-20181008T180358\",\n",
    "                                   train_instance_type='ml.m5.large',\n",
    "                                   output_path=output_prefix,\n",
    "                                   sagemaker_session=AWSSession)\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=nFeatures,\n",
    "                      predictor_type='binary_classifier',\n",
    "                      mini_batch_size=1000000,\n",
    "                      num_factors=64,\n",
    "                      epochs=25)\n",
    "\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm.deploy(instance_type='ml.t2.medium', initial_instance_count=1, endpoint_name = 'AnimeFactorizationMachine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Interacting with our deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor = sagemaker.predictor.RealTimePredictor(endpoint = 'AnimeFactorizationMachine', \n",
    "                                                     sagemaker_session= AWSSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fm_serializer(data):\n",
    "    js = {'instances': []}\n",
    "    for row in data:\n",
    "        js['instances'].append({'features': row.tolist()})\n",
    "    #print js\n",
    "    return json.dumps(js)\n",
    "\n",
    "fm_predictor.content_type = 'application/json'\n",
    "fm_predictor.serializer = fm_serializer\n",
    "fm_predictor.deserializer = sagemaker.predictor.json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following code results in predictions on our test set (or at least 10 of its elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = fm_predictor.predict(X_test[1000:1010].toarray())\n",
    "print(result)\n",
    "print (Y_test[1000:1010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want reccomendations for a particular user though? We need to do some more work. Right now our factorization machine is able to predict whether or not users that we trained our model on will like certain shows. Because our dataset doesn't contain all the users on myanimelist, we need to find a way for it to make predictions about arbitrary users.\n",
    "\n",
    "To do this, we essentially use collaborative filtering. We find the user who's most similar to the inputted user, then make predictions for that user. Because our model returns a probability that the user will like a certain show, we just take the shows with the highest probabilities as our recommendation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This returns a sparse row vector representing the shows and (normalize) scores a user has watched\n",
    "# We will use this to find the user most similar to the user we're looking for.\n",
    "#\n",
    "\n",
    "def getUserMtrx(username):\n",
    "    jk = Jikan()\n",
    "    userList = jk.user(username = username, request = 'animelist')['anime']\n",
    "    userMtrx = sp.sparse.lil_matrix((1,nAnime))\n",
    "    L = []\n",
    "    for show in userList:\n",
    "        L.append(show['score'])\n",
    "    m = np.mean(L)\n",
    "    s = np.std(L)\n",
    "    for show in userList:\n",
    "        if show['mal_id'] in animeMap:\n",
    "            userMtrx[0, animeMap[show['mal_id']]] = (show['score']-m)/s\n",
    "                \n",
    "    return sp.sparse.csr_matrix(userMtrx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This creates a sparse pivot matrix representing all the users and shows we trained on\n",
    "# We will use this to find the user most similar to the user we're looking for.\n",
    "#\n",
    "\n",
    "def createPivotSparse(pdDF):\n",
    "    d = {}\n",
    "    a = 0\n",
    "    X = sp.sparse.lil_matrix((nUsers,nAnime)).astype('float32')\n",
    "    userId = 0\n",
    "    for row in pdDF.itertuples():\n",
    "        if row[1] not in d:\n",
    "            d[row[1]] = a\n",
    "            userId = a\n",
    "            a = a +1\n",
    "        else:\n",
    "            userId = d[row[1]]\n",
    "        X[userId, animeMap[row[2]]] = row[3]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# this returns the number of the user who's most similar to the inputted user\n",
    "# It does this by computing the cosine similarity between the user and the other users\n",
    "#\n",
    "def mostSimilarUser(username):\n",
    "    userList = getUserMtrx(username)\n",
    "    \n",
    "    X = createPivotSparse(scaledList)\n",
    "    \n",
    "    userSimilarity = cosine_similarity(X, userList, dense_output = False)\n",
    "    result = userSimilarity.argmax()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This returns the sparse output corresponding to the shows the user has not watched\n",
    "#\n",
    "def getAnimeList(username, mostSimilarUser):\n",
    "    jk = Jikan()\n",
    "    userList = jk.user(username = username, request = 'animelist')['anime']\n",
    "    showsWatched = {}\n",
    "    for show in userList:\n",
    "        if show['mal_id'] in animeMap:\n",
    "            Aid = animeMap[show['mal_id']]\n",
    "            showsWatched[Aid] = show['title']\n",
    "        \n",
    "    unwatchedTop = []\n",
    "    for p in range(1,5):\n",
    "        top_anime = jk.top(type='anime', page=p)['top']\n",
    "        for show in top_anime:\n",
    "            if show['mal_id'] in animeMap:\n",
    "                aId = animeMap[show['mal_id']]\n",
    "                if aId not in showsWatched:\n",
    "                    unwatchedTop.append(aId)\n",
    "                \n",
    "    X = sp.sparse.lil_matrix((len(unwatchedTop), nFeatures)).astype('float32')\n",
    "    for x in range(len(unwatchedTop)):\n",
    "        X[x, 0] = 1\n",
    "        X[x, unwatchedTop[x]] = 1\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This runs our prediction algorithm on the sparse output corresponding to the shows\n",
    "# the user has not watched and returns the top 5 results\n",
    "#\n",
    "def indicesOfBestAnime(X):\n",
    "    weights = []\n",
    "    l = X.shape[0]\n",
    "    indexOfResult = 0\n",
    "    for i in range(0,l,5):\n",
    "        pred = fm_predictor.predict(X[i:i+5].toarray())['predictions']\n",
    "        for j,x in enumerate(pred):\n",
    "            if weights == []:\n",
    "                weights.append((x['score'],i+j))\n",
    "            if x['score'] > weights[0][0]:\n",
    "                \n",
    "                weights.append((x['score'],i+j))\n",
    "                if len(weights) > 5:\n",
    "                    weights = sorted(weights, key = (lambda x: x[0]))[1:6]\n",
    "                else:\n",
    "                    weights = sorted(weights, key = (lambda x: x[0]))\n",
    "        del(pred)\n",
    "    \n",
    "    \n",
    "    return list(map(lambda x: x[1], weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# given an index of a show, this returns it's name\n",
    "#\n",
    "def indexToAnime(X, i):\n",
    "    jk = Jikan()\n",
    "    a = X[i].nonzero()\n",
    "    for val in animeMap.keys():\n",
    "        if animeMap[val] == a[1][1]:\n",
    "            showId = val\n",
    "            break\n",
    "    \n",
    "    showinfo = jk.anime(showId)\n",
    "    showname = showinfo['title']\n",
    "    return showname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This packages all the previous functions\n",
    "#\n",
    "def animeRecommend(username):\n",
    "    i = mostSimilarUser(username)\n",
    "    X = getAnimeList(username, i)\n",
    "    indices = indicesOfBestAnime(X)\n",
    "    animeNames = list(map((lambda i: indexToAnime(X,i)), indices))\n",
    "    \n",
    "    return animeNames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = animeRecommend('dakness989')\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
