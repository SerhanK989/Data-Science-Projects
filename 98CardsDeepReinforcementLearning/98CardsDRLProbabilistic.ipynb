{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning on 98Cards\n",
    "\n",
    "![Image of n8Cards](https://i.stack.imgur.com/GJ3vy.png)\n",
    "98 Cards is a simple solitaire-esque game. The objective is to play all 98 cards (which are numbered 2-99) onto the 4 stacks. Two of the stacks only let you play a card greater than the previously played card, and the other two only let you play a card less than the previously played card. The kicker is that is that if the difference between two cards on the stack is exactly 10, it can be played on that stack regardless of whether its greater or less than the card on the stack. \n",
    "\n",
    "In this notebook I implement an AI that learns to play 98 cards using Deep Reinforcement Learning. The file n8Cards contains a barebones version of the game programmed by me. The reward function used is: -100 for an invalid move, +n for a valid move where n is the number of cards played so far. This incentives the AI to play the most cards it can. \n",
    "\n",
    "First, we import some necessary tools. Numpy helps with some processing along the way and Tensorflow does most of the work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import n8Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "stateSize = 13 #4 piles, 8 cards, cardsRemaining\n",
    "actionSize = 32 #4 piles x 8 cards\n",
    "\n",
    "#training variables, these can all be adjusted to get different results\n",
    "gamma = .95\n",
    "learning_rate = .003\n",
    "trainingSize = 100000\n",
    "epsilon = .0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I define some helper functions. The first turns our game state into a numpy array so it can be inputted into the neural network.\n",
    "\n",
    "The second turns our actionList into a vector where the i'th input is 1 if that action is playable. We use the resulting vector to elementwise multiply the output of our neural net so it only allows us to play valid actions.\n",
    "\n",
    "Normally, a softmax on the result of the neural network would return a nice distribution, but we filtered out a lot of the output, so there are weird edge cases where we take the softmax of a small number of small values, or all negative values. After being softmaxed, this leads to the value zero having a large weight after being softmaxed and causes us to play invalid moves. We define this function so we can filter out the invalid actions and scale the values appropriately so our softmax function only returns a distribution over the valid actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boardToVec(curBoard):\n",
    "    O = (curBoard.observable)\n",
    "    O.sort()\n",
    "    V = [curBoard.cardsRemaining]+O\n",
    "    return np.array(V)\n",
    "\n",
    "a = n8Cards.Board(1)\n",
    "print(boardToVec(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionsToVec(actionList):\n",
    "    result = np.zeros(actionSize)\n",
    "    for (stack,card) in actionList:\n",
    "        result[stack*8+card] = 1\n",
    "    return result\n",
    "\n",
    "actionsToVec([(3,7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAssistant(V):\n",
    "    L = list(list(V)[0])\n",
    "    allNeg = True\n",
    "    for x in L:\n",
    "        if x > epsilon:\n",
    "            allNeg = False\n",
    "            break\n",
    "    if allNeg:\n",
    "        return V*-100000000000.0\n",
    "    else:\n",
    "        return V*100000000000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have our neural net, it takes in a vector of size 13 and pushes it through 5 fully connected layers. The first four all use a relu activation function, and the last one uses a tanh activation function because we don't want valid moves to have a 0 in their place, otherwise we might get a 0 vector after filtering invalid actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, stateSize], name = \"inputs\")\n",
    "\n",
    "legalActions = tf.placeholder(tf.float32, [None, actionSize], name = \"legalActions\")\n",
    "actionsTaken = tf.placeholder(tf.float32, [None, actionSize], name = \"actionsTaken\")\n",
    "discountedNormedRewards = tf.placeholder(tf.float32, [None, ], name = \"discountedNormedRewards\")\n",
    "\n",
    "\n",
    "fcLayer1 = tf.contrib.layers.fully_connected(inputs = inputs, num_outputs = 48, activation_fn = tf.nn.relu,\n",
    "                                             weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "\n",
    "fcLayer2 = tf.contrib.layers.fully_connected(inputs = fcLayer1, num_outputs = 96, activation_fn = tf.nn.relu,\n",
    "                                            weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "fcLayer3 = tf.contrib.layers.fully_connected(inputs = fcLayer2, num_outputs = 192, activation_fn = tf.nn.relu,\n",
    "                                            weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "fcLayer4 = tf.contrib.layers.fully_connected(inputs = fcLayer3, num_outputs = 96, activation_fn = tf.nn.relu,\n",
    "                                            weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "fcLayer5 = tf.contrib.layers.fully_connected(inputs = fcLayer4, num_outputs = actionSize, activation_fn = tf.nn.tanh,\n",
    "                                            weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "filtered = tf.multiply(fcLayer5, legalActions)\n",
    "\n",
    "filteredScaled = tf.py_func(filterAssistant, [filtered], tf.float32)\n",
    "\n",
    "softmax = tf.nn.softmax(filteredScaled)\n",
    "\n",
    "negLogProb = tf.nn.softmax_cross_entropy_with_logits_v2(logits = filtered, labels = actionsTaken)\n",
    "loss = tf.reduce_mean(negLogProb * discountedNormedRewards)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use Deep Reinforcement Learning to train the NN. The outline of what we do is below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create the NN\n",
    "For episode in range(max_episodes):\n",
    "    episode + 1\n",
    "    reset environment\n",
    "    For each step:\n",
    "        Choose action a\n",
    "        Perform action a\n",
    "        Store s, a, r\n",
    "        If done and after a number of episodes:\n",
    "            Calculate sum reward\n",
    "            Calculate gamma Gt\n",
    "            Optimize\n",
    "            Reset stores (states, actions, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxScore = 0\n",
    "allScores = []\n",
    "\n",
    "actionList = []\n",
    "stateList = []\n",
    "rewards = []\n",
    "possibleActions = []\n",
    "totalRewards = []\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print('Starting Training')\n",
    "\n",
    "for step in range(trainingSize):\n",
    "    \n",
    "    board = n8Cards.Board()\n",
    "    \n",
    "    actions = board.getActionList()\n",
    "    \n",
    "    while len(actions) != 0:\n",
    "        #play the game, storing everything\n",
    "        stateList.append(boardToVec(board).reshape([1,13]))\n",
    "        \n",
    "        feedDict =  {inputs: boardToVec(board).reshape([1,13]), legalActions: actionsToVec(actions).reshape(1, 32)}\n",
    "        \n",
    "        \n",
    "        dist = sess.run(softmax,feed_dict = feedDict)\n",
    "        \n",
    "        possibleActions.append(actionsToVec(actions).reshape(1, 32))\n",
    "        \n",
    "        \n",
    "        actionIndex = np.random.choice(range(dist.shape[1]), p=dist.ravel())\n",
    "        \n",
    "        actionVec = np.zeros(32)\n",
    "        actionVec[actionIndex] = 1\n",
    "        actionList.append(actionVec)\n",
    "        \n",
    "        card = actionIndex%8\n",
    "        stack = actionIndex// 8\n",
    "        \n",
    "        reward = board.playAction(stack,card)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        actions = board.getActionList()\n",
    "        \n",
    "    totalScore = np.sum(rewards)\n",
    "    allScores.append(totalScore)    \n",
    "        \n",
    "    #OPTIMIZE\n",
    "\n",
    "    averageRewards = np.mean(allScores)\n",
    "\n",
    "    maxScore = np.max(allScores)\n",
    "\n",
    "    #discounting+normalizing\n",
    "    discountedRewards = np.zeros_like(rewards)\n",
    "    cumulative = 0\n",
    "\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        cumulative = cumulative * gamma + rewards[i]\n",
    "        discountedRewards[i] = cumulative\n",
    "\n",
    "    mean = np.mean(discountedRewards)\n",
    "    std = np.std(discountedRewards)\n",
    "    discountedRewards = (discountedRewards - mean) / (std)\n",
    "\n",
    "    Oloss,_ = sess.run([loss, optimizer], feed_dict={inputs: np.vstack(np.array(stateList)),\n",
    "                                                                 legalActions: np.array(possibleActions).reshape(len(possibleActions),32),\n",
    "                                                                  actionsTaken: np.vstack(np.array(actionList)),\n",
    "                                                                  discountedNormedRewards: discountedRewards})\n",
    "\n",
    "    #Reset\n",
    "    rewards = []\n",
    "    stateList = []\n",
    "    actionList = []\n",
    "    possibleActions = []\n",
    "\n",
    "    if step % 1000 == 999:\n",
    "        print(np.mean(allScores[-500:]))\n",
    "        print(maxScore)\n",
    "    \n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this model to have our machine play 98 Cards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAction(board, actions):  \n",
    "    \n",
    "    bV = boardToVec(board).reshape([1,13])\n",
    "    \n",
    "    feedDict =  {inputs: boardToVec(board).reshape([1,13]), legalActions: actionsToVec(actions).reshape(1, 32)}\n",
    "           \n",
    "    dist = sess.run(softmax,feed_dict = feedDict)\n",
    "                \n",
    "    actionIndex = np.random.choice(range(dist.shape[1]), p=dist.ravel())\n",
    "    \n",
    "    card = actionIndex%8\n",
    "    stack = actionIndex// 8\n",
    "    \n",
    "    return card, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(seed = 0):\n",
    "    if seed == 0:\n",
    "        board = n8Cards.Board()\n",
    "    else:\n",
    "        board = n8Cards.Board(seed)\n",
    "        \n",
    "    actions = board.getActionList()\n",
    "    \n",
    "    while len(actions) != 0:\n",
    "        card, stack = findAction(board, actions)\n",
    "        board.playAction(stack, card)\n",
    "        actions = board.getActionList()\n",
    "    return board.score\n",
    "        \n",
    "playGame()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
